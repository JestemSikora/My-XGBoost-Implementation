{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc49546e",
   "metadata": {},
   "source": [
    "# My XGBoost implementation\n",
    "\n",
    "This time, plan is a little diffrent. I will exceptionally not implement the code from scratch, because I would probably need C++ efficiency.\n",
    "Main goal of these mini-projects was to have handy and not complicated \"notebook\" written in Python to refresh my knowledge later in my journey. XGBoost is more computationally difficult, so it would disrupt the initial assumption for this project.\n",
    "\n",
    "Of course, I could implement it on smaller dataset, but does it make sense when XGBoost was made for really large datasets?\n",
    "\n",
    "So, this time I will do only two steps.\n",
    "\n",
    "    1. Notes for XGBoost\n",
    "    2. Implementation from libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb4a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get to work!\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's get to work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963db238",
   "metadata": {},
   "source": [
    "## 1. Notes\n",
    "    A. Main idea\n",
    "    B. How it works\n",
    "    C. Math behind the algorithm\n",
    "    D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d148479",
   "metadata": {},
   "source": [
    "### A. Main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadee02",
   "metadata": {},
   "source": [
    "**XGBoost** is an algorithm that builds many decision trees, where <u> each new tree corrects the errors of the previous ones.</u> <br>\n",
    "It is based on Gradient Boosting, but introduces optimizations that make it faster and more effective on large and complex datasets. <br>\n",
    "\n",
    "Key improvments are:\n",
    "\n",
    "    * Regularization to reduce overfitting -> adds Î» penalties to prevent overfitting and improve generalization.\n",
    "  \n",
    "    * Efficient handling of missing values -> automatically learns the best path for missing data, no need for preprocessing.\n",
    "  \n",
    "    * Parallelization for speed  -> computes tree splits in parallel, making training much faster on large datasets.\n",
    "  \n",
    "    * Sparsity awareness and memory optimization -> optimized for sparse data and memory usage with efficient data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b267c",
   "metadata": {},
   "source": [
    "### B. How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea996ee",
   "metadata": {},
   "source": [
    "1) Start with an initial prediction (e.g., 0.5 for log-loss classification, or the mean of y for regression).\n",
    "2) Calculating <u>Residuals</u> for every example (Residual = prediction - real_data).\n",
    "3) Building first leaf that contains all training samples.\n",
    "4) For the first leaf, we calculate <u>Similarity Score</u>\n",
    "   \n",
    "   ![](Sim_score.png)\n",
    "\n",
    "   in easier language...\n",
    "\n",
    "\n",
    "   ![](sim_score_2.png) \n",
    "\n",
    "5) After that, we split the residuals into two groups in many possible ways. Suppose we have m residuals: in the first scenario, one residual goes to the left leaf and the remaining m-1 to the right; in the next scenario, two residuals go left and m-2 go right; and so on.\n",
    "6) For each scenario we calculate <u>Gain</u>\n",
    "   \n",
    "   ![](gain.png) \n",
    "\n",
    "   ![](gain2.png)\n",
    "\n",
    "   We chceck gain in every scenario, and pick the biggest one. The winner becomes a new node.\n",
    "\n",
    "7) This pattern continues, making new nodes until it reaches the max depth.\n",
    "\n",
    "8) However, we pick Î³ - parameter for prunning tree. Now we check if gain - Î³ is negative or not. If it is, we prune a node, and check if prevous node is also negative.\n",
    "\n",
    "9) Calculating output values\n",
    "    \n",
    "    ![](output_val.png)\n",
    "\n",
    "\n",
    "10) Update predictions: new prediction = old prediction + ðœ‚â‹… (leaf weight), where ðœ‚ is the learning rate.\n",
    "\n",
    "    ![](n_pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebc91b",
   "metadata": {},
   "source": [
    "quick summary at 23:57 minute -> https://www.youtube.com/watch?v=OtD8wVaFm6E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f2ee1",
   "metadata": {},
   "source": [
    "### C. Algorithm in Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1415a94",
   "metadata": {},
   "source": [
    "### D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61365099",
   "metadata": {},
   "source": [
    "* âœ… When you have large and complex datasets\n",
    "\n",
    "* âœ… When data contains missing values\n",
    "\n",
    "* âœ… When you need high accuracy \n",
    "\n",
    "* âœ… Works with diffrent types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b35bda",
   "metadata": {},
   "source": [
    "## 2. Implementation from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8b9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af713b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>cancer_stage</th>\n",
       "      <th>family_history</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>bmi</th>\n",
       "      <th>cholesterol_level</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>asthma</th>\n",
       "      <th>cirrhosis</th>\n",
       "      <th>other_cancer</th>\n",
       "      <th>treatment_type</th>\n",
       "      <th>survived</th>\n",
       "      <th>treatment_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.462069</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Chemotherapy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>Female</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Stage III</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>Stage III</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Former Smoker</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.47</td>\n",
       "      <td>Female</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>No</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Chemotherapy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33</td>\n",
       "      <td>Male</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>No</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender      country cancer_stage family_history  smoking_status  \\\n",
       "0  0.60    Male       Sweden      Stage I            Yes  Passive Smoker   \n",
       "1  0.46  Female  Netherlands    Stage III            Yes  Passive Smoker   \n",
       "2  0.61  Female      Hungary    Stage III            Yes   Former Smoker   \n",
       "3  0.47  Female      Belgium      Stage I             No  Passive Smoker   \n",
       "4  0.33    Male   Luxembourg      Stage I             No  Passive Smoker   \n",
       "\n",
       "        bmi  cholesterol_level  hypertension  asthma  cirrhosis  other_cancer  \\\n",
       "0  0.462069           0.326667             0       0          1             0   \n",
       "1  0.868966           0.866667             1       1          0             0   \n",
       "2  0.965517           0.786667             1       1          0             0   \n",
       "3  0.931034           0.606667             1       1          0             0   \n",
       "4  0.127586           0.186667             0       0          0             0   \n",
       "\n",
       "  treatment_type  survived  treatment_time  \n",
       "0   Chemotherapy         0        0.621572  \n",
       "1        Surgery         1        0.440585  \n",
       "2       Combined         0        0.341865  \n",
       "3   Chemotherapy         0        0.475320  \n",
       "4       Combined         0        0.407678  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset \n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\wikto\\OneDrive\\Dokumenty\\all-datasets\\Lung Cancer.csv\")\n",
    "\n",
    "# Prepering Data\n",
    "\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# 1. Getting days passed since diagnosis\n",
    "df[\"diagnosis_date\"] = pd.to_datetime(df[\"diagnosis_date\"])\n",
    "df[\"end_treatment_date\"] = pd.to_datetime(df[\"end_treatment_date\"])\n",
    "\n",
    "treatment_time = (df[\"diagnosis_date\"] - df[\"end_treatment_date\"]).abs()\n",
    "\n",
    "df[\"treatment_time\"] = treatment_time\n",
    "df[\"treatment_time\"] = df[\"treatment_time\"].dt.days.astype('int')\n",
    "\n",
    "df = df.drop([\"diagnosis_date\", \"end_treatment_date\"], axis=1)\n",
    "\n",
    "\n",
    "# 2. Fixing category type of data\n",
    "\n",
    "df[\"gender\"] = df[\"gender\"].astype('category')\n",
    "df[\"country\"] = df[\"country\"].astype('category')\n",
    "df[\"cancer_stage\"] = df[\"cancer_stage\"].astype('category')\n",
    "df[\"smoking_status\"] = df[\"smoking_status\"].astype('category')\n",
    "df[\"treatment_type\"] = df[\"treatment_type\"].astype('category')\n",
    "df[\"family_history\"] = df[\"family_history\"].astype('category')\n",
    "\n",
    "\n",
    "# 3. Data Normalization\n",
    "\n",
    "# Normalizacja nie miaÅ‚a dla XGBoosta Å¼adnego znazenia \n",
    "scal = MinMaxScaler()\n",
    "df[[\"age\"]] = scal.fit_transform(df[[\"age\"]])\n",
    "df[[\"bmi\"]] = scal.fit_transform(df[[\"bmi\"]])\n",
    "df[[\"cholesterol_level\"]] = scal.fit_transform(df[[\"cholesterol_level\"]])\n",
    "df[[\"treatment_time\"]] = scal.fit_transform(df[[\"treatment_time\"]])\n",
    "\n",
    "# Splitting to X's and y\n",
    "y = df[\"survived\"]\n",
    "X = df.drop(\"survived\", axis=1)\n",
    "\n",
    "#X = pd.get_dummies(X, drop_first=False)\n",
    "\n",
    "# Splitting to train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ae7a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataset into DMatrix structure\n",
    "xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wikto\\OneDrive\\Dokumenty\\Implementations of ML\\XGBoost\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"early_stopping_rounds\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# Creating XGBoost Model\n",
    "\n",
    "# 1. Picking parameters\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic', # type of learning task\n",
    "    'max_depth': 4,  # maximum depth level of a tree\n",
    "    'learning_rate': 0.01,\n",
    "    'tree_method': 'hist', # How trees are built - 'hist' uses histogram-based algorithm (faster on large datasets)\n",
    "    'scale_pos_weight':3.56, # it improved recall, precision and F1-score <- scales according to how much more often 0's occur, usefull when data is inbalanced\n",
    "    'min_child_weight': 8, # minimum number of weights that can lay in single leaf -  prevents overfitting by requiring more samples per leaf\n",
    "    # 'early_stopping_rounds': 50 # after 50 round without mutch change in loss function, trainning stops\n",
    "}\n",
    "n = 1000 # number of maximum iterations\n",
    "model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc4afc",
   "metadata": {},
   "source": [
    "### XGBoost â€“ wpÅ‚yw wielkoÅ›ci parametrÃ³w i zalecane zakresy\n",
    "\n",
    "| Parametr              | MaÅ‚a wartoÅ›Ä‡                                                                 | DuÅ¼a wartoÅ›Ä‡                                                                 | Zalecany zakres startowy |\n",
    "|-----------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|---------------------------|\n",
    "| **objective**         | Nie dotyczy skali (binary:logistic = klasyfikacja binarna, output 0-1).      | Nie dotyczy.                                                                 | 'binary:logistic'         |\n",
    "| **max_depth**         | PÅ‚ytsze drzewa, prostsze reguÅ‚y, mniejsze ryzyko przeuczenia, ale moÅ¼liwe niedouczenie. | GÅ‚Ä™bsze drzewa, wiÄ™ksza zdolnoÅ›Ä‡ dopasowania, wiÄ™ksze ryzyko przeuczenia, wolniejsze trenowanie. | 3 â€“ 8 (czÄ™sto 4â€“6)        |\n",
    "| **learning_rate**     | Wolniejsze uczenie, stabilniejsze, potrzeba wiÄ™cej drzew, zwykle lepsza generalizacja. | Szybsze uczenie, mniej drzew, wiÄ™ksze ryzyko przeuczenia i niestabilnoÅ›ci.   | 0.01 â€“ 0.1                |\n",
    "| **tree_method**       | Algorytm budowy drzew (`hist` = szybki, oszczÄ™dny w pamiÄ™ci). Skala nie dotyczy. | Skala nie dotyczy.                                                           | 'hist' lub 'approx'       |\n",
    "| **scale_pos_weight**  | Klasy traktowane jako w przybliÅ¼eniu zbalansowane.                           | WiÄ™ksze znaczenie klasy rzadkiej, roÅ›nie recall, ale moÅ¼e spaÅ›Ä‡ precision.   | â‰ˆ (liczba_neg / liczba_pos) |\n",
    "| **min_child_weight**  | LiÅ›cie mogÄ… powstawaÄ‡ z maÅ‚Ä… liczbÄ… prÃ³bek â€“ model elastyczny, wiÄ™ksze ryzyko przeuczenia. | Wymaga wiÄ™cej prÃ³bek w liÅ›ciu â€“ silniejsza regularyzacja, mniej podziaÅ‚Ã³w, mniejsze ryzyko przeuczenia. | 1 â€“ 10                    |\n",
    "| **early_stopping_rounds** | Szybsze zatrzymanie, ryzyko zbyt wczesnego stopu przy szumiÄ…cych danych.   | DÅ‚uÅ¼sze trenowanie, mniejsze ryzyko przedwczesnego zatrzymania.              | 20 â€“ 100                  |\n",
    "| **num_boost_round**   | MaÅ‚o drzew â€“ model moÅ¼e byÄ‡ niedouczony (szczegÃ³lnie przy niskim learning_rate). | DuÅ¼o drzew â€“ wiÄ™ksza pojemnoÅ›Ä‡ modelu, ryzyko przeuczenia (chyba Å¼e uÅ¼ywasz early stopping). | 500 â€“ 2000 (z early stopping) |\n",
    "\n",
    "---\n",
    "\n",
    "### Interakcje\n",
    "- NiÅ¼szy `learning_rate` + wyÅ¼szy `num_boost_round` = czÄ™sta bezpieczna kombinacja.  \n",
    "- WyÅ¼szy `max_depth` zwykle wymaga wyÅ¼szego `min_child_weight` lub dodatkowej regularyzacji.  \n",
    "- `scale_pos_weight` dobieraj wg proporcji klas, potem koryguj pod precision/recall.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66cddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.49      0.60    138962\n",
      "           1       0.22      0.51      0.30     39038\n",
      "\n",
      "    accuracy                           0.49    178000\n",
      "   macro avg       0.50      0.50      0.45    178000\n",
      "weighted avg       0.66      0.49      0.54    178000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking model precision, recall, f1-score\n",
    "y_pred = model.predict(xgb_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'{class_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3edfa4",
   "metadata": {},
   "source": [
    "As you can see, the model does not perform well on this type of data. Problem probably lies in the very small number of samples containing survived patients. Also, due to the quite big amount of categorical data, CatBoost would likely perform better than XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
