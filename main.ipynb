{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc49546e",
   "metadata": {},
   "source": [
    "# My XGBoost implementation\n",
    "\n",
    "This time, plan is a little diffrent. I will exceptionally not implement the code from scratch, because I would probably need C++ efficiency.\n",
    "Main goal of these mini-projects was to have handy and not complicated \"notebook\" written in Python to refresh my knowledge later in my journey. XGBoost is more computationally difficult, so it would disrupt the initial assumption for this project.\n",
    "\n",
    "Of course, I could implement it on smaller dataset, but does it make sense when XGBoost was made for really large datasets?\n",
    "\n",
    "So, this time I will do only two steps.\n",
    "\n",
    "    1. Notes for XGBoost\n",
    "    2. Implementation from scikt-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb4a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get to work!\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's get to work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963db238",
   "metadata": {},
   "source": [
    "## 1. Notes\n",
    "    A. Main idea\n",
    "    B. How it works\n",
    "    C. Algorithm in Math\n",
    "    D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d148479",
   "metadata": {},
   "source": [
    "### A. Main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadee02",
   "metadata": {},
   "source": [
    "**XGBoost** is an algorithm that builds many decision trees, where <u> each new tree corrects the errors of the previous ones.</u> <br>\n",
    "It is based on Gradient Boosting, but introduces optimizations that make it faster and more effective on large and complex datasets. <br>\n",
    "\n",
    "Key improvments are:\n",
    "\n",
    "    * Regularization to reduce overfitting -> adds Î» penalties to prevent overfitting and improve generalization.\n",
    "  \n",
    "    * Efficient handling of missing values -> automatically learns the best path for missing data, no need for preprocessing.\n",
    "  \n",
    "    * Parallelization for speed  -> computes tree splits in parallel, making training much faster on large datasets.\n",
    "  \n",
    "    * Sparsity awareness and memory optimization -> optimized for sparse data and memory usage with efficient data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b267c",
   "metadata": {},
   "source": [
    "### B. How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea996ee",
   "metadata": {},
   "source": [
    "1) Start with an initial prediction (e.g., 0.5 for log-loss classification, or the mean of y for regression).\n",
    "2) Calculating <u>Residuals</u> for every example (Residual = prediction - real_data).\n",
    "3) Building first leaf that contains all training samples.\n",
    "4) For the first leaf, we calculate <u>Similarity Score</u>\n",
    "   \n",
    "   ![](Sim_score.png)\n",
    "\n",
    "   in easier language...\n",
    "\n",
    "\n",
    "   ![](sim_score_2.png) \n",
    "\n",
    "5) After that, we split the residuals into two groups in many possible ways. Suppose we have m residuals: in the first scenario, one residual goes to the left leaf and the remaining m-1 to the right; in the next scenario, two residuals go left and m-2 go right; and so on.\n",
    "6) For each scenario we calculate <u>Gain</u>\n",
    "   \n",
    "   ![](gain.png) \n",
    "\n",
    "   ![](gain2.png)\n",
    "\n",
    "   We chceck gain in every scenario, and pick the biggest one. The winner becomes a new node.\n",
    "\n",
    "7) This pattern continues, making new nodes until it reaches the max depth.\n",
    "\n",
    "8) However, we pick Î³ - parameter for prunning tree. Now we check if gain - Î³ is negative or not. If it is, we prune a node, and check if prevous node is also negative.\n",
    "\n",
    "9) Calculating output values\n",
    "    \n",
    "    ![](output_val.png)\n",
    "\n",
    "\n",
    "10) Update predictions: new prediction = old prediction + ðœ‚â‹… (leaf weight), where ðœ‚ is the learning rate.\n",
    "\n",
    "    ![](n_pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebc91b",
   "metadata": {},
   "source": [
    "quick summary at 23:57 minute -> https://www.youtube.com/watch?v=OtD8wVaFm6E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f2ee1",
   "metadata": {},
   "source": [
    "### C. Algorithm in Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1415a94",
   "metadata": {},
   "source": [
    "### D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61365099",
   "metadata": {},
   "source": [
    "* âœ… When you have large and complex datasets\n",
    "\n",
    "* âœ… When data contains missing values\n",
    "\n",
    "* âœ… When you need high accuracy \n",
    "\n",
    "* âœ… Works with diffrent types of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
