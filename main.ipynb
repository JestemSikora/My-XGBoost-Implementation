{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc49546e",
   "metadata": {},
   "source": [
    "# My XGBoost implementation\n",
    "\n",
    "This time, plan is a little diffrent. I will exceptionally not implement the code from scratch, because I would probably need C++ efficiency.\n",
    "Main goal of these mini-projects was to have handy and not complicated \"notebook\" written in Python to refresh my knowledge later in my journey. XGBoost is more computationally difficult, so it would disrupt the initial assumption for this project.\n",
    "\n",
    "Of course, I could implement it on smaller dataset, but does it make sense when XGBoost was made for really large datasets?\n",
    "\n",
    "So, this time I will do only two steps.\n",
    "\n",
    "    1. Notes for XGBoost\n",
    "    2. Implementation from libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb4a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get to work!\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's get to work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963db238",
   "metadata": {},
   "source": [
    "## 1. Notes\n",
    "    A. Main idea\n",
    "    B. How it works\n",
    "    C. Math behind the algorithm\n",
    "    D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d148479",
   "metadata": {},
   "source": [
    "### A. Main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadee02",
   "metadata": {},
   "source": [
    "**XGBoost** is an algorithm that builds many decision trees, where <u> each new tree corrects the errors of the previous ones.</u> <br>\n",
    "It is based on Gradient Boosting, but introduces optimizations that make it faster and more effective on large and complex datasets. <br>\n",
    "\n",
    "Key improvments are:\n",
    "\n",
    "    * Regularization to reduce overfitting -> adds λ penalties to prevent overfitting and improve generalization.\n",
    "  \n",
    "    * Efficient handling of missing values -> automatically learns the best path for missing data, no need for preprocessing.\n",
    "  \n",
    "    * Parallelization for speed  -> computes tree splits in parallel, making training much faster on large datasets.\n",
    "  \n",
    "    * Sparsity awareness and memory optimization -> optimized for sparse data and memory usage with efficient data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b267c",
   "metadata": {},
   "source": [
    "### B. How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea996ee",
   "metadata": {},
   "source": [
    "1) Start with an initial prediction (e.g., 0.5 for log-loss classification, or the mean of y for regression).\n",
    "2) Calculating <u>Residuals</u> for every example (Residual = prediction - real_data).\n",
    "3) Building first leaf that contains all training samples.\n",
    "4) For the first leaf, we calculate <u>Similarity Score</u>\n",
    "   \n",
    "   ![](Sim_score.png)\n",
    "\n",
    "   in easier language...\n",
    "\n",
    "\n",
    "   ![](sim_score_2.png) \n",
    "\n",
    "5) After that, we split the residuals into two groups in many possible ways. Suppose we have m residuals: in the first scenario, one residual goes to the left leaf and the remaining m-1 to the right; in the next scenario, two residuals go left and m-2 go right; and so on.\n",
    "6) For each scenario we calculate <u>Gain</u>\n",
    "   \n",
    "   ![](gain.png) \n",
    "\n",
    "   ![](gain2.png)\n",
    "\n",
    "   We chceck gain in every scenario, and pick the biggest one. The winner becomes a new node.\n",
    "\n",
    "7) This pattern continues, making new nodes until it reaches the max depth.\n",
    "\n",
    "8) However, we pick γ - parameter for prunning tree. Now we check if gain - γ is negative or not. If it is, we prune a node, and check if prevous node is also negative.\n",
    "\n",
    "9) Calculating output values\n",
    "    \n",
    "    ![](output_val.png)\n",
    "\n",
    "\n",
    "10) Update predictions: new prediction = old prediction + 𝜂⋅ (leaf weight), where 𝜂 is the learning rate.\n",
    "\n",
    "    ![](n_pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebc91b",
   "metadata": {},
   "source": [
    "quick summary at 23:57 minute -> https://www.youtube.com/watch?v=OtD8wVaFm6E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f2ee1",
   "metadata": {},
   "source": [
    "### C. Algorithm in Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1415a94",
   "metadata": {},
   "source": [
    "### D. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61365099",
   "metadata": {},
   "source": [
    "* ✅ When you have large and complex datasets\n",
    "\n",
    "* ✅ When data contains missing values\n",
    "\n",
    "* ✅ When you need high accuracy \n",
    "\n",
    "* ✅ Works with diffrent types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b35bda",
   "metadata": {},
   "source": [
    "## 2. Implementation from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8b9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af713b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>cancer_stage</th>\n",
       "      <th>family_history</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>bmi</th>\n",
       "      <th>cholesterol_level</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>asthma</th>\n",
       "      <th>cirrhosis</th>\n",
       "      <th>other_cancer</th>\n",
       "      <th>treatment_type</th>\n",
       "      <th>survived</th>\n",
       "      <th>treatment_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.462069</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Chemotherapy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>Female</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Stage III</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>Stage III</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Former Smoker</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.47</td>\n",
       "      <td>Female</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>No</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Chemotherapy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33</td>\n",
       "      <td>Male</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>No</td>\n",
       "      <td>Passive Smoker</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender      country cancer_stage family_history  smoking_status  \\\n",
       "0  0.60    Male       Sweden      Stage I            Yes  Passive Smoker   \n",
       "1  0.46  Female  Netherlands    Stage III            Yes  Passive Smoker   \n",
       "2  0.61  Female      Hungary    Stage III            Yes   Former Smoker   \n",
       "3  0.47  Female      Belgium      Stage I             No  Passive Smoker   \n",
       "4  0.33    Male   Luxembourg      Stage I             No  Passive Smoker   \n",
       "\n",
       "        bmi  cholesterol_level  hypertension  asthma  cirrhosis  other_cancer  \\\n",
       "0  0.462069           0.326667             0       0          1             0   \n",
       "1  0.868966           0.866667             1       1          0             0   \n",
       "2  0.965517           0.786667             1       1          0             0   \n",
       "3  0.931034           0.606667             1       1          0             0   \n",
       "4  0.127586           0.186667             0       0          0             0   \n",
       "\n",
       "  treatment_type  survived  treatment_time  \n",
       "0   Chemotherapy         0        0.621572  \n",
       "1        Surgery         1        0.440585  \n",
       "2       Combined         0        0.341865  \n",
       "3   Chemotherapy         0        0.475320  \n",
       "4       Combined         0        0.407678  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset \n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\wikto\\OneDrive\\Dokumenty\\all-datasets\\Lung Cancer.csv\")\n",
    "\n",
    "# Prepering Data\n",
    "\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# 1. Getting days passed since diagnosis\n",
    "df[\"diagnosis_date\"] = pd.to_datetime(df[\"diagnosis_date\"])\n",
    "df[\"end_treatment_date\"] = pd.to_datetime(df[\"end_treatment_date\"])\n",
    "\n",
    "treatment_time = (df[\"diagnosis_date\"] - df[\"end_treatment_date\"]).abs()\n",
    "\n",
    "df[\"treatment_time\"] = treatment_time\n",
    "df[\"treatment_time\"] = df[\"treatment_time\"].dt.days.astype('int')\n",
    "\n",
    "df = df.drop([\"diagnosis_date\", \"end_treatment_date\"], axis=1)\n",
    "\n",
    "\n",
    "# 2. Fixing category type of data\n",
    "\n",
    "df[\"gender\"] = df[\"gender\"].astype('category')\n",
    "df[\"country\"] = df[\"country\"].astype('category')\n",
    "df[\"cancer_stage\"] = df[\"cancer_stage\"].astype('category')\n",
    "df[\"smoking_status\"] = df[\"smoking_status\"].astype('category')\n",
    "df[\"treatment_type\"] = df[\"treatment_type\"].astype('category')\n",
    "df[\"family_history\"] = df[\"family_history\"].astype('category')\n",
    "\n",
    "\n",
    "# 3. Data Normalization\n",
    "\n",
    "# Normalizacja nie miała dla XGBoosta żadnego znazenia \n",
    "scal = MinMaxScaler()\n",
    "df[[\"age\"]] = scal.fit_transform(df[[\"age\"]])\n",
    "df[[\"bmi\"]] = scal.fit_transform(df[[\"bmi\"]])\n",
    "df[[\"cholesterol_level\"]] = scal.fit_transform(df[[\"cholesterol_level\"]])\n",
    "df[[\"treatment_time\"]] = scal.fit_transform(df[[\"treatment_time\"]])\n",
    "\n",
    "# Splitting to X's and y\n",
    "y = df[\"survived\"]\n",
    "X = df.drop(\"survived\", axis=1)\n",
    "\n",
    "#X = pd.get_dummies(X, drop_first=False)\n",
    "\n",
    "# Splitting to train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ae7a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataset into DMatrix structure\n",
    "xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wikto\\OneDrive\\Dokumenty\\Implementations of ML\\XGBoost\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"early_stopping_rounds\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# Creating XGBoost Model\n",
    "\n",
    "# 1. Picking parameters\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic', # type of learning task\n",
    "    'max_depth': 4,  # maximum depth level of a tree\n",
    "    'learning_rate': 0.01,\n",
    "    'tree_method': 'hist', # How trees are built - 'hist' uses histogram-based algorithm (faster on large datasets)\n",
    "    'scale_pos_weight':3.56, # it improved recall, precision and F1-score <- scales according to how much more often 0's occur, usefull when data is inbalanced\n",
    "    'min_child_weight': 8, # minimum number of weights that can lay in single leaf -  prevents overfitting by requiring more samples per leaf\n",
    "    # 'early_stopping_rounds': 50 # after 50 round without mutch change in loss function, trainning stops\n",
    "}\n",
    "n = 1000 # number of maximum iterations\n",
    "model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc4afc",
   "metadata": {},
   "source": [
    "### XGBoost – wpływ wielkości parametrów i zalecane zakresy\n",
    "\n",
    "| Parametr              | Mała wartość                                                                 | Duża wartość                                                                 | Zalecany zakres startowy |\n",
    "|-----------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|---------------------------|\n",
    "| **objective**         | Nie dotyczy skali (binary:logistic = klasyfikacja binarna, output 0-1).      | Nie dotyczy.                                                                 | 'binary:logistic'         |\n",
    "| **max_depth**         | Płytsze drzewa, prostsze reguły, mniejsze ryzyko przeuczenia, ale możliwe niedouczenie. | Głębsze drzewa, większa zdolność dopasowania, większe ryzyko przeuczenia, wolniejsze trenowanie. | 3 – 8 (często 4–6)        |\n",
    "| **learning_rate**     | Wolniejsze uczenie, stabilniejsze, potrzeba więcej drzew, zwykle lepsza generalizacja. | Szybsze uczenie, mniej drzew, większe ryzyko przeuczenia i niestabilności.   | 0.01 – 0.1                |\n",
    "| **tree_method**       | Algorytm budowy drzew (`hist` = szybki, oszczędny w pamięci). Skala nie dotyczy. | Skala nie dotyczy.                                                           | 'hist' lub 'approx'       |\n",
    "| **scale_pos_weight**  | Klasy traktowane jako w przybliżeniu zbalansowane.                           | Większe znaczenie klasy rzadkiej, rośnie recall, ale może spaść precision.   | ≈ (liczba_neg / liczba_pos) |\n",
    "| **min_child_weight**  | Liście mogą powstawać z małą liczbą próbek – model elastyczny, większe ryzyko przeuczenia. | Wymaga więcej próbek w liściu – silniejsza regularyzacja, mniej podziałów, mniejsze ryzyko przeuczenia. | 1 – 10                    |\n",
    "| **early_stopping_rounds** | Szybsze zatrzymanie, ryzyko zbyt wczesnego stopu przy szumiących danych.   | Dłuższe trenowanie, mniejsze ryzyko przedwczesnego zatrzymania.              | 20 – 100                  |\n",
    "| **num_boost_round**   | Mało drzew – model może być niedouczony (szczególnie przy niskim learning_rate). | Dużo drzew – większa pojemność modelu, ryzyko przeuczenia (chyba że używasz early stopping). | 500 – 2000 (z early stopping) |\n",
    "\n",
    "---\n",
    "\n",
    "### Interakcje\n",
    "- Niższy `learning_rate` + wyższy `num_boost_round` = częsta bezpieczna kombinacja.  \n",
    "- Wyższy `max_depth` zwykle wymaga wyższego `min_child_weight` lub dodatkowej regularyzacji.  \n",
    "- `scale_pos_weight` dobieraj wg proporcji klas, potem koryguj pod precision/recall.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66cddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.49      0.60    138962\n",
      "           1       0.22      0.51      0.30     39038\n",
      "\n",
      "    accuracy                           0.49    178000\n",
      "   macro avg       0.50      0.50      0.45    178000\n",
      "weighted avg       0.66      0.49      0.54    178000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking model precision, recall, f1-score\n",
    "y_pred = model.predict(xgb_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'{class_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3edfa4",
   "metadata": {},
   "source": [
    "As you can see, the model does not perform well on this type of data. Problem probably lies in the very small number of samples containing survived patients. Also, due to the quite big amount of categorical data, CatBoost would likely perform better than XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
